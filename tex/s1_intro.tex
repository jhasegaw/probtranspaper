\section{Introduction}

\IEEEPARstart{A}{utomatic} speech recognition (ASR) has the potential to provide
database access, simultaneous translation, and text/voice messaging
services to anybody, in any language, dramatically reducing linguistic
barriers to economic success.  To date, ASR has failed to achieve its
potential, because successful ASR requires very large labeled
corpora;
%. Current methods require about 1000 hours of transcribed
%speech per language, transcribed at a cost of about 6000 hours of
%human labor;
the human transcribers must be computer-literate, and
they must be native speakers of the language being transcribed.
%In
%many languages, recruiting dozens of computer-literate
%native speakers is impractical.
{\color{blue} Large corpora are beyond the resources of most
  under-resourced language communities; we have found that
  transcribing even one hour of speech may be beyond the reach of
  communities that lack large-scale government funding.  In order to
  create the databases reported in this paper, for example, we sought
  paid native transcribers, at a competitive wage, for the 68
  languages in which we have untranscribed audio data.  We found
  transcribers willing to work in only eleven of those languages, of
  which only seven finished the task.}

Instead of recruiting native transcribers in search of a perfect
reference transcript, this paper proposes the use of probabilistic
transcripts.  A probabilistic transcript is a probability mass
function, $\rho_\Phi(\phi)$, specifying, as a real number between $0$ and
$1$, the probability that any particular phonetic transcript $\phi$
is the correct transcript of the utterance.  Prior to this work,
machine learning has almost always assumed that the training dataset
contains either deterministic transcripts
($\rho_{DT}(\phi)\in\left\{0,1\right\}$, commonly called ``supervised
training'') or completely untranscribed utterances (commonly called
``unsupervised training,'' in which case we assume that $\rho_{LM}(\phi)$
is given by some {\em a priori} language model).  This article
proposes that, even in the absence of a deterministic transcript,
there may be auxiliary sources of information that can be compiled to
create a probabilistic transcript with entropy lower than that
of the language model, and that machine learning methods applied to
the probabilistic transcript are able to make use of its reduced
entropy in order to learn a better ASR.  In particular,
this paper considers three useful auxiliary sources of information:
\begin{enumerate}
\item SELF-TRAINING: ASR pre-trained in other languages is used to
  transcribe unlabeled training data in the target language.
\item MISMATCHED CROWDSOURCING: Human crowd workers who don't speak
  the target language are asked to transcribe it as if it were a
  sequence of nonsense syllables.
\item EEG DISTRIBUTION CODING: Humans who do not speak the target
  language are asked to listen to its extracted syllables, and their
  EEG responses are interpreted as a probability mass function over
  possible phonetic transcripts.
\end{enumerate}

