\section{Introduction}

\IEEEPARstart{A}{utomatic} speech recognition (ASR) has the potential to provide
database access, simultaneous translation, and text/voice messaging
services to anybody, in any language, dramatically reducing linguistic
barriers to economic success.  To date, ASR has failed to achieve its
potential, because successful ASR requires very large labeled
corpora. Current methods require about 1000 hours of transcribed
speech per language, transcribed at a cost of about 6000 hours of
human labor; the human transcribers must be computer-literate, and
they must be native speakers of the language being transcribed.
%In
%many languages, recruiting dozens of computer-literate
%native speakers is impractical.
Doing so is beyond the resources of most under-resourced language
communities; we have found that transcribing even one hour of speech
is beyond the reach of some under-resourced language communities.
In order to create the databases reported in this paper, we sought
paid native transcribers for the seventy languages in which we have
untranscribed audio data.  We found transcribers willing to work in
only eleven of those languages, of which only seven finished the task.

Instead of recruiting native transcribers in search of a perfect
reference transcript, this paper proposes the use of probabilistic
transcripts.  A probabilistic transcript is a probability mass
function, $\rho_\Phi(\phi)$, specifying, as a real number between $0$ and
$1$, the probability that any particular phonetic transcript $\phi$
is the correct transcript of the utterance.  Prior to this work,
machine learning has almost always assumed that the training dataset
contains either deterministic transcripts
($\rho_{DT}(\phi)\in\left\{0,1\right\}$, commonly called ``supervised
training'') or completely untranscribed utterances (commonly called
``unsupervised training,'' in which case we assume that $\rho_{LM}(\phi)$
is given by some {\em a priori} language model).  This article
proposes that, even in the absence of a deterministic transcript,
there may be auxiliary sources of information that can be compiled to
create a probabilistic transcript with entropy lower than that
of the language model, and that machine learning methods applied to
the probabilistic transcript are able to make use of its reduced
entropy in order to learn a better ASR.  In particular,
this paper considers three useful auxiliary sources of information:
\begin{enumerate}
\item SELF-TRAINING: ASR pre-trained in other languages is used to
  transcribe unlabeled training data in the target language.
\item MISMATCHED CROWDSOURCING: Human crowd workers who don't speak
  the target language are asked to transcribe it as if it were a
  sequence of nonsense syllables.
\item EEG DISTRIBUTION CODING: Humans who do not speak the target
  language are asked to listen to its extracted syllables, and their
  EEG responses are interpreted as a probability mass function over
  possible phonetic transcripts.
\end{enumerate}

