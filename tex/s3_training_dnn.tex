\subsection{Neural Networks}

The NN acoustic model is $\pi_{X_t^{\ell}|S_t^\ell}(v|j,\theta)\propto y_t^\ell(j)$,
\begin{equation}
y_t^\ell(j)=\frac{1}{c_j}\frac{\exp\left(w_j^Th_t(v,w_{vh})\right)}
{\sum_k \exp\left(w_k^Th_t(v,w_{vh})\right)}
\end{equation}
whose parameters $\theta=\left\{c_j,w_j,w_{vh}\right\}$ include the
senone priors $c_j$, the softmax weight vectors $w_j$, and the
parameters defining the hidden nodes $h_t(v,w_{vh})$.  NNs are
trained by using a GMM-HMM to compute an initial senone posterior,
$\pi_{S_t^{\ell}}(j|x^\ell,\theta)$, then minimizing the cross-entropy
between the estimated senone posterior and the neural network output
$y_{t}^\ell(j)$,
%.  The cross entropy is measured as
%\begin{equation}
%  H(S^\ell\Vert Y^\ell)=-\sum_{t=1}^T \sum_{j} \pi(s_t^\ell=j) \ln y_{t}^\ell(j)
%  \label{eq:dnn_train}
%\end{equation}
%The gradient of Eq.~(\ref{eq:dnn_train}) with respect to its model
%parameters is
using gradient descent in the direction
\begin{equation}
  -\nabla_\theta H(S^\ell\Vert Y^\ell)=
  \sum_{t=1}^T\sum_j\frac{\pi_{S_t^{\ell}}(j|x^\ell,\theta)}{y_t^\ell(j)}
  \nabla_\theta y_t^\ell(j)
  \label{eq:dnn_dt}
\end{equation}
NN training with deterministic transcripts is improved by
quantizing $\pi_{S_t^\ell}(j|x^\ell,\theta)\rightarrow\left\{0,1\right\}$
using forced alignment~\cite{Dahl2012}. Preliminary experiments showed
that forced alignment also improves the accuracy of NNs trained from
probabilistic transcripts: the best path through the PT, and the
best alignment of the resulting senones to the waveform, were both
computed using forced alignment.  The resulting best senone string was
used to train a NN using Eq.~(\ref{eq:dnn_dt}).

