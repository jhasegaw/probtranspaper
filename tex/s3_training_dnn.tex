\subsection{Neural Networks}

The NN acoustic model is $\pi_{X_t^{\ell}|S_t^\ell}(v|j,\theta)\propto y_t^\ell(j)$,
\begin{equation}
y_t^\ell(j)=\frac{1}{c_j}\frac{\exp\left(w_j^Th_t(v,w_{vh})\right)}
{\sum_k \exp\left(w_k^Th_t(v,w_{vh})\right)}
\end{equation}
whose parameters $\theta=\left\{c_j,w_j,w_{vh}\right\}$ include the
senone priors $c_j$, the softmax weight vectors $w_j$, and the
parameters defining the hidden nodes $h_t(v,w_{vh})$.  NNs are
trained by using a GMM-HMM to compute an initial senone posterior,
$\pi_{S_t^{\ell}}(j|x^\ell,\theta)$, then minimizing the cross-entropy
between the estimated senone posterior and the neural network output
$y_{t}^\ell(j)$,
using gradient descent in the direction
\begin{equation}
  -\nabla_\theta H(S^\ell\Vert Y^\ell)=
  \sum_{t=1}^T\sum_j\frac{\pi_{S_t^{\ell}}(j|x^\ell,\theta)}{y_t^\ell(j)}
  \nabla_\theta y_t^\ell(j)
  \label{eq:dnn_dt}
\end{equation}
Preliminary experiments showed that forced alignment improves the
accuracy of NNs trained from probabilistic transcripts: the best path
through the PT, and the best alignment of the resulting senones to the
waveform, were both computed using forced alignment.  The resulting
best senone string was used to train a NN using Eq.~(\ref{eq:dnn_dt}).

