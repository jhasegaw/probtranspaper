\subsection{Maximum {\em A Posteriori} Adaptation}
\label{sec:adaptation}

PT adaptation starts from a cross-lingual ASR,
%Training from PTs can be improved by starting from a multilingual ASR,
and adapts its parameters to PTs in the target language.  The
Bayesian framework for maximum {\em a posteriori} (MAP) estimation
has been widely applied to GMM and HMM parameter estimation problems
such as speaker adaptation~\cite{gauvain1994maximum}.
Formally, for an unseen target language, denote its acoustic
observations $x = ( x_1^1, \ldots, x_{T}^L )$, and its acoustic model
parameter set as $\theta$, then the MAP parameters are defined as:
\begin{equation}
  \theta_{\mathrm{MAP}}  = \argmax_{\theta} \pi(\theta | x) 
= \argmax_{\theta} \pi( x | \theta ) \pi(\theta)
\label{eq:map}
\end{equation}
\noindent where $\pi(\theta)$ is the product of conjugate
prior distributions, centered at the parameters of a cross-lingual
baseline ASR.  In a GMM-HMM, the acoustic model 
%\[
%\pi(x_t^\ell|s_t^\ell =j,\theta )=
%\sum_{k=1}^K c_{jk}
%\mathcal{N}
%\left(x_t^\ell|\mu_{jk},v_{jk}
%\right)
%\]
%\noindent
is computed by choosing a Gaussian component, $G_t^\ell$, whose mixture
weight is $c_{jk}=\pi_{G_t^{\ell}|S_t^{\ell}}(k|j)$, and whose mean vector
and covariance matrix are $\mu_{jk}$ and $\Sigma_{jk}$.  Maximum
likelihood trains these parameters by computing
$\gamma^{\ell}_t(j,k)=\pi_{S_t^{\ell},G_t^{\ell}}(j,k|x^\ell,\theta)$,
then accumulating weighted average acoustic frames with weights given
by $\gamma_{t}^\ell(j,k)$. Segmental K-means quantizes
$\pi_{S_t^\ell}(j|x^\ell,\theta)\rightarrow\left\{0,1\right\}$ using forced
alignment, then proceeds identically.  MAP adaptation assigns, to each
parameter, a conjugate prior $\pi(\theta)$ with mode equal to
$\bar\theta$ (the parameters of the cross-lingual baseline), and with a
confidence hyperparameter $\tau_\theta$, resulting in re-estimation
formulae that are linearly interpolated between the baseline
parameters $\bar\theta$ and the statistics of the adaptation data, for
example:
\begin{equation}
c_{jk}'=\frac{\tau_c\bar{c}_{jk}+\sum_{\ell,t}\gamma_{t}^\ell(j,k)}
{\sum_{n}\left(\tau_c\bar{c}_{jn}+
  \sum_{\ell,t}\gamma_{t}^\ell(j,n)\right)}
%~~~
%\mu_{jk}'=\frac{\tau_\mu\bar{\mu}_{jk}+\sum_{\ell,t}\gamma_{t}^\ell(j,k)x_t^\ell}
%   {\tau_\mu+\sum_{\ell,t}\gamma_{t}^\ell(j,k)}
   %,~~~
%v_{jk}'=\frac{\tau_v\bar{v}_{jk}+\sum_{\ell,t}\gamma_{\ell t}(j,k)(x_t^\ell-\mu_{jk})^2}
%{\tau_v+\sum_{\ell,t}\gamma_{\ell t}(j,k)}
\end{equation}
%In our setting, the initial value of $\bar{\mu}_{jk}$ is obtained from
%the multilingual baseline model, and $\mu_{jk}'$ eventually converges
%to a model for the target language data.


