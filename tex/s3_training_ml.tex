\subsection{Maximum Likelihood Training}

Consider two observation-conditional sequence distributions
$\pi(s,\phi|x,\theta)$ and $\pi(s,\phi|x,\theta')$, with parameter
vectors $\theta$ and $\theta'$ respectively.  The cross-entropy
between these distributions is~\cite{Dempster77}:
\begin{align}
  H\left(\theta\Vert\theta'\right) &=
  \sum_{s,\phi} \pi(s,\phi|x,\theta)
  \ln \pi(s,\phi|x,\theta')\\
  &=   \sum_{s,\phi} \pi(s,\phi|x,\theta)
  \left(\ln \pi(s,\phi,x|\theta')-\ln \pi(x|\theta')\right)\\
  &=  Q\left(\theta,\theta'\right)-{\mathcal L}\left(\theta'\right)
  \label{eq:crossentropy}
\end{align}
where the data log likelihood, ${\mathcal L}\left(\theta'\right)$, and
the expectation maximization (EM) quality function,
$Q\left(\theta,\theta'\right)$, are defined by
\begin{align}
  {\mathcal L}\left(\theta'\right) &= \ln \pi(x|\theta')
  \label{eq:loglikelihood}\\
  Q\left(\theta,\theta'\right)
  &=
  \sum_{s,\phi} \pi(s,\phi|x,\theta)\ln \pi(s,\phi,x|\theta')
   \label{eq:Qfunction}
\end{align}
The Kullback-Leibler divergence between $\pi(s,\phi|x,\theta)$ and
$\pi(s,\phi|x,\theta')$ is $D\left(\theta\Vert\theta'\right)=
H\left(\theta\Vert\theta'\right)-H\left(\theta\Vert\theta\right)$.
Since $D\left(\theta\Vert\theta'\right)\ge 0$~\cite{Shannon49},
\begin{equation}
  {\mathcal L}\left(\theta'\right)-{\mathcal L}\left(\theta\right)\ge
  Q\left(\theta,\theta'\right)-
  Q\left(\theta,\theta\right)
  \label{eq:LgeQ}
\end{equation}
Given any initial parameter vector $\theta_n$, the expectation
maximization (EM) algorithm finds $\theta_{n+1}=\argmax
Q(\theta_n,\theta')$, thereby maximizing the minimum increment in
${\mathcal L}(\theta)$.  For GMM-HMMs, the quality function
$Q\left(\theta,\theta'\right)$ is convex and can be analytically
maximized; for DNN-HMMs it is non-convex, but can be maximized using
gradient ascent.
%% above paragraph is the first appearance of "DNN", which has not yet
%% been defined (though "NN" has).

The probability $\pi(x,s,\phi|\theta)$ is computed by composing the
following three weighted FSTs:
\begin{align}
  \mathbf{PT}&:\phi^\ell\rightarrow\phi^\ell/ \rho(\phi^\ell)\\
  \mathbf{HC}&:\phi^\ell\rightarrow s^\ell/ \pi(s^\ell|\pi^\ell,\theta)\\
  \mathbf{AM}&:s^\ell\rightarrow s^\ell/ \pi(x^\ell|s^\ell,\phi^\ell,\theta)
\end{align}
where the notation has the following meaning.  The probabilistic
transcription, $\mathbf{PT}$, is an FST that maps any phone string
$\phi^\ell\in\mathbb{\Phi}^*$ to itself.  This mapping is
deterministic and reflexive, but comes with a path cost determined by
the transcription probability $\rho(\phi^\ell)$, as exemplified in
Fig.~\ref{fig:pt}.  The HMM-expansion transducer, $\mathbf{HC}$, maps
any phone sequence $\phi^\ell$ to a state sequence $s^\ell$.  This FST
is the composition of the $\mathbf{H}$ and $\mathbf{C}$ transducers
defined by~\cite{Mohri2002}.  This mapping is non-deterministic, and
the path cost is determined by the HMM transition weights distribution
$a_{ij}=\pi(s_t^\ell =j|s_{t-1}^\ell =i,\phi^\ell,\theta)$:
\begin{equation}
  \pi(s^\ell|\phi,\theta)=\prod_{\ell=1}^L\prod_{t=1}^T
  a_{s_{t-1}^\ell s_t^\ell}
\end{equation}
The acoustic modeling transducer $\mathbf{AM}$ maps any state sequence
to itself.  This mapping is deterministic and reflexive, but comes
with a path cost determined by the acoustic modeling probability
\begin{equation}
  \pi(x^\ell|s^\ell,\phi^\ell,\theta)=\prod_{\ell=1}^L\prod_{t=1}^T
  \pi(x_t^\ell|s_t^\ell,\theta^\ell)
\end{equation}
The joint probability $\pi(\phi^\ell,s^\ell,x^\ell|\theta)$ is
computed by composing the FSTs, then finding the total cost of the
path through
$\left(\mathbf{AM}\circ\mathbf{HC}\circ\mathbf{PT}\right)$ with input
string $\phi^\ell$ and output string $s^\ell$.  The posterior
probability $\pi(\phi^\ell,s^\ell|x^\ell,\theta)$ is computed by
pushing the composed FST, then finding the total cost of the path
through
$\textrm{push}\left(\mathbf{AM}\circ\mathbf{HC}\circ\mathbf{PT}\right)$.

%% single-sentence paragraph (consider merging into above or below
%% paragraphs)
The parameter vector $\theta$ includes the HMM transition
probabilities, $a_{ij}=\pi(s_t^\ell =j|s_{t-1}^\ell =i,\phi,\theta)$,
and the parameters of the acoustic model
$b_j(x_t^\ell)=\pi(x_t^\ell|s_t^\ell=j,\theta)$.

Computing the analytical maximum or gradient of
$Q\left(\theta,\theta'\right)$ requires summation over all possible
state alignments $s\sim S$.  The summation can be performed
efficiently using the Baum-Welch algorithm, but experimental tests
reported in this paper did not do so, for reasons described in the
next subsection.
