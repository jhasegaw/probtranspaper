\subsection{Maximum Likelihood Training}

Consider two observation-conditional sequence distributions
$\pi(s,\phi|x,\theta)$ and $\pi(s,\phi|x,\theta')$, with parameter
vectors $\theta$ and $\theta'$ respectively.  The cross-entropy
between these distributions is:
\begin{align}
  H\left(\theta\Vert\theta'\right) &=
  -\sum_{s,\phi} \pi(s,\phi|x,\theta)
  \ln \pi(s,\phi|x,\theta')\\
  %&=   \sum_{s,\phi} \pi(s,\phi|x,\theta)
  %\left(\ln \pi(x|\theta')-\ln \pi(x,s,\phi |\theta')\right)\\
  &=  {\mathcal L}\left(\theta'\right)-Q\left(\theta,\theta'\right)
  \label{eq:crossentropy}
\end{align}
where the data log likelihood, ${\mathcal L}\left(\theta'\right)$, and
the expectation maximization (EM) quality function,
$Q\left(\theta,\theta'\right)$~\cite{Dempster77}, are
\begin{align}
  {\mathcal L}\left(\theta'\right) &= \ln \pi(x|\theta')
  \label{eq:loglikelihood}\\
  Q\left(\theta,\theta'\right)
  &=
  \sum_{s,\phi} \pi(s,\phi|x,\theta)\ln \pi(x,s,\phi |\theta')
   \label{eq:Qfunction}
\end{align}
Cross-entropy is bounded ($H\left(\theta\Vert\theta'\right)\ge
H\left(\theta\Vert\theta\right)$), therefore
\begin{equation}
  {\mathcal L}\left(\theta'\right)-{\mathcal L}\left(\theta\right)\ge
  Q\left(\theta,\theta'\right)-
  Q\left(\theta,\theta\right)
  \label{eq:LgeQ}
\end{equation}
Given any initial parameter vector $\theta_n$, the EM
algorithm finds $\theta_{n+1}=\argmax_{\theta'}
Q(\theta_n,\theta')$, thereby maximizing the minimum increment in
${\mathcal L}(\theta)$.  For GMM-HMMs, the quality function
$Q\left(\theta,\theta'\right)$ is concave and can be analytically
maximized; for NN-HMMs it is non-concave, but can be maximized using
gradient ascent~\cite{Bengio92}.

The probability $\pi(x,s,\phi|\theta)$ is computed by composing the
following three weighted FSTs:
\begin{align}
  H&:s^\ell\rightarrow s^\ell/ \pi(x^\ell|s^\ell,\phi^\ell,\theta)\\
  C&:s^\ell\rightarrow \phi^\ell/ \pi(s^\ell|\phi^\ell,\theta)\\
  PT&:\phi^\ell\rightarrow\phi^\ell/ \rho(\phi^\ell)
\end{align}
where the notation has the following meaning.  The probabilistic
transcript, $PT$, is an FST that maps any phone string
to itself.  This mapping is deterministic
and reflexive, but comes with a path cost determined by the
transcription probability $\rho(\phi^\ell)$, as exemplified in
Fig.~\ref{fig:pt}.  The context transducer, $C$, maps any senone
sequence $s^\ell$ to a phone sequence $\phi^\ell$~\cite{mohri2008speech}.
This mapping is stochastic, and the path cost is determined by
the HMM transition weights
%$a_{ij}=\pi_{S_t^{\ell}|S_{t-1}^{\ell}}(j|i,\phi^\ell,\theta)$:
\begin{equation}
  \pi(s^\ell|\phi,\theta)=%\prod_{\ell=1}^L
  \prod_{t=1}^T
  %  a_{s_{t-1}^\ell s_t^\ell}
  \pi(s_t^{\ell}\vert s_{t-1}^{\ell},\phi^\ell,\theta)
\end{equation}
The acoustic model, $H$, maps any senone sequence to
itself.  This mapping is deterministic and reflexive, but comes with a
path cost determined by the acoustic modeling probability
\begin{equation}
  \pi(x^\ell|s^\ell,\phi^\ell,\theta)=%\prod_{\ell=1}^L
  \prod_{t=1}^T
  \pi(x_t^\ell|s_t^\ell,\theta)
\end{equation}
The posterior probability
$\pi(s,\phi|x,\theta)$ is computed by
composing the FSTs, pushing toward the initial state
(normalizing so that probabilities sum to one),
then finding the total cost of the path through
$\textsc{push}\left(H\circ C\circ PT\right)$
with input string $s$ and output string $\phi$.
The analytical maximum of $Q\left(\theta,\theta'\right)$
can be computed efficiently using
the Baum-Welch algorithm, but experiments reported in this
paper did not do so, for reasons described in the next subsection.
