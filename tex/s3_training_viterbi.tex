\subsection{Segmental K-Means Training}

The EM quality function, $Q(\theta,\theta')$,
has properties that make it undesirable as an optimizer for ${\mathcal{L}}$.
Suppose, as often happens, that there is a poor phone
sequence, $\phi^p$, that is highly unlikely given the correct
parameter vector $\theta^*$, meaning that $\pi(x,s,\phi^p|\theta^*)$
is very low.  Suppose that the initial parameter vector, $\theta$, is
less discriminative, so that
$\pi(x,s,\phi^p|\theta)>\pi(x,s,\phi^p|\theta^*)$.
Indeed, the best speech recognizer is a parameter vector $\theta^*$
that completely rules out poor transcripts, setting
$\pi(x,s,\phi^p|\theta^*)=0$; but in this case
$Q(\theta,\theta^*)=-\infty$.
It is therefore not possible for the EM algorithm to start with
parameters $\theta$ that allow $\phi^p$, and to find parameters $\theta^*$
that rule out $\phi^p$.
With probabilistic
transcription, this problem is quite common: if the
human transcribers fail to rule out $\phi^p$ (e.g., because the
correct and incorrect transcripts are perceptually
indistinguishable in the language of the transcribers), then the EM
algorithm will also never learn to rule out $\phi^p$.

EM's inability to learn zero-valued probabilities can be ameliorated
by using the segmental K-means algorithm~\cite{Juang1990}, which
bounds ${\mathcal L}(\theta')$ as
$\mathcal{L}(\theta')\ge
R(\theta,\theta')$:
\begin{align}
  R(\theta,\theta') &= \ln \pi(x,s^*(\theta),\phi^*(\theta)|\theta')\\
  s^*(\theta),\phi^*(\theta)&= \argmax_{s,\phi} \pi(s,\phi|x,\theta)
\end{align}
Given an initial parameter vector $\theta$, therefore, it is possible
to find a new parameter vector $\theta'$ with higher likelihood by
computing its maximum-likelihood senone sequence and phone sequence
$s^*(\theta),\phi^*(\theta)$, and by maximizing $\theta'$ with respect to
$s^*(\theta)$ and $\phi^*(\theta)$.
Maximizing
$R(\theta,\theta')$ rather than $Q(\theta,\theta')$ is useful for
probabilistic transcription because it reduces the importance of poor
phonetic transcripts.
