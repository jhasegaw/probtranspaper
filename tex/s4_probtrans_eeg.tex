\subsection{Estimating Misperceptions from Electrocortical Responses}
\label{sec:eegchanmod}

The misperception G2P described in Section~\ref{sec:MC} was estimated
using a combination of mismatched and deterministic transcripts of
non-target languages. However, with a small amount
of transcribed data in the utterance language, it is possible
to estimate the misperception G2P using electrocortical measurements
of non-native speech perception. In this approach, the misperception G2P
is decomposed into two separate transducers,
a misperception transducer $\rho(\psi|\phi)$, and an
annotation-language G2P $\rho(\lambda|\psi)$:
\begin{equation}
  \rho(\lambda|\phi)\approx\sum_{\psi}\rho(\lambda|\psi)\rho(\psi|\phi)
\end{equation}
where $\phi$ is a phone string in the utterance language, $\psi$ is a
phone string in the annotation language, and $\lambda$ is an
orthographic string in the annotation language.  $\rho(\lambda|\psi)$
is an inverted G2P in the annotation language, e.g., trained on the
CMU dictionary of American English pronunciations~\cite{Lenzo1995}.
$\rho(\psi|\phi)$ is the mismatch transducer, specifying the
probability that a phone string $\phi$ in the utterance language will
be mis-heard as the annotation-language phone string $\psi$. 

In principle, the mismatch transducer could be computed empirically from
a phone confusion matrix, if experimental data on phone confusions
were available for all phones in the target language, and those data
were based on responses from a listener with the same language background
as the crowd worker transcribers. These goals are hard to meet. 
An alternative is to use distinctive feature representations
(originally proposed to characterize the perceptual and phonological
natural classes of phonemes~\cite{Jakobson52}) to predict misperceptions
based on differences between the distinctive feature values of 
annotation- and utterance-language phones. Given the assumption that 
every distinctive feature shared by phones $\phi$ and $\psi$ 
independently increases their confusion probability, their confusion 
probability can be expressed as
%\begin{equation}
%  \rho(\psi|\phi)\propto \exp\left(-\sum_{k=1}^K
%  w_k\delta\left(f_k(\psi)\ne f_k(\phi)\right)\right)
%  \label{eq:dfdist}
%\end{equation}
%where $f_k(\phi)$ is the $k^{\textrm{th}}$ feature of phoneme $\phi$,
%and $\delta(\cdot)\in\left\{0,1\right\}$ is the unit indicator
%function. 
\begin{equation}
  \rho(\psi|\phi)\propto \exp\left(-\sum_{k=1}^K
  w_k(\phi,\psi)\right)
  \label{eq:dfdist}
\end{equation}
where $w_k(\phi,\psi)$ is
%the contribution of the $k^{\textrm{th}}$ feature in the 
%misperception of the of phone $\phi$ as phone $\psi$. If a feature is 
%perceived similarly across the two languages its contribution would be 
%lower when when the two phones share the same value of that
smaller if $\phi$ and $\psi$ share the $k^{\textrm{th}}$ distinctive
feature. The assumption of independence is a simplifying assumption,
given that many distinctive features have overlapping acoustic
correlates. For example, the frequencies of the {\em two} lowest
resonances of the vocal tract (the primary cues for vowel identity) are
determined by articulatory gestures of the lips, jaw and tongue that are
commonly represented by {\em three or more} distinctive features
(e.g., height, backness, rounding, and advanced tongue root). Moreover,
the weights $w_k$ will probably also depend on properties of the speaker
and listener (language, dialect, and idiolect), but data to train such a
rich model do not exist.

However, a reasonable approximate model can be learned by assuming
that $w_k$ depend only on information about the listener, which can be
incorporated via measurements of electrocortical activity. In
particular, the weights $w_k$ of the distinctive features can be set
based on similarity of electrocortical responses (measured using EEG)
as determined by a classifier trained to compute distinctive feature
representations from electrocortical responses to the listener's native
language phones.
Thus, suppose a listener first hears phones $\phi=\psi$ in the native
language, EEG response signals $y$ are recorded, and a bank of
binary classifiers $g_k(y)$ are trained to label the distinctive
features $f_k(\phi)$~\cite{Liberto15}.  Second, the same listener
hears phones $\phi\ne\psi$ in a new language, and EEG response
signals $y$ are recorded;
%Thus, given a set of EEG response signals $y$
%recorded when a listener hears audio corresponding to phone $\phi$ in
%the annotation language, whose $k^\textrm{th}$ distinctive feature is
%$f_k(\phi)$, and supposing that $g_k(y)$ is the output of a
%binary classifier trained to detect $f_k(\phi)$ based on speech in the
%listener's native language\cite{Liberto15},
then the contributions in
Eq.~(\ref{eq:dfdist}) can be estimated as
\begin{equation}
  w_k(\phi,\psi) = -\ln\Pr\left\{g_k(y)= f_k(\phi)\right\}
  \label{eq:eegdist}
\end{equation}
