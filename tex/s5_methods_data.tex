%\subsection{Data}
\section{Audio Data and Mismatched Crowdsourcing}
\label{sec:data}

Speech data were extracted from publicly available podcasts~\cite{SBS}
hosted in 68 different languages.  In order to generate test corpora
(in which it is possible to measure phone error rate), advertisements
were posted at the University of Illinois seeking native speakers
willing to transcribe speech in any of these 68 languages.  Of the ten
transcribers who responded, six people were each able to complete one
hour of speech transcription (the other four dropped out).  One
additional language was transcribed by workers recruited at $I^2R$ in
Singapore, yielding a total of seven languages with native
transcripts suitable for testing an ASR: Arabic (arb), Cantonese
(yue), Dutch (nld), Hungarian (hun), Mandarin (cmn), Swahili (swh) and
Urdu (urd).
It is desirable to test the ideas in this paper with corpora larger
than one hour per language, but larger corpora involve problems
orthogonal to the purposes of this paper, e.g., the Babel corpora
contain telephone speech, and therefore contain far more acoustic
background noise than the podcast corpora used in this paper.

The podcasts contain utterances interspersed with segments of music
and English. A GMM-based language identification system was
developed in order to isolate
regions that correspond mostly to the target language, which
were then split into 5-second
segments to enable easy labeling by the native transcribers.
Native transcribers were asked to omit any 5-second clips that
contained significant music, noise, English, or speech from multiple
speakers. Resulting transcripts covered 45 minutes of speech in Urdu
and 1 hour of speech in the remaining six languages. The orthographic
transcripts for these clips were then converted into phonemic
transcripts using language-specific dictionaries and G2P mappings.
In order to make it possible to transfer ASR from
training languages (which have native transcripts) to a test
language (that has no native transcripts), the phone set must be
standardized across all languages; for this purpose, the phone set
was based on the international phonetic alphabet
(IPA;~\cite{ipa1993}).  Similarly, in order to transfer ASR from
training languages to a test language, the training transcriptions
must be converted to phonemes using a grapheme-to-phoneme transducer
(G2P).  G2Ps were therefore assumed to be available in all training
languages, but not in the test language.  Since these G2Ps are only
used for training and not test languages, five of them (Arabic,
Dutch, Hungarian, Cantonese and Mandarin) were trained using lexical
resources, and only two (Urdu and Swahili) were constructed using
the zero-resource knowledge-based method described in
Sec.~\ref{sec:trainwithlm}.  English words in each transcript are
identified and converted to phones with an English G2P trained using
CMUdict~\cite{Lenzo1995}, then other words are converted into phonetic
transcripts using language-dependent dictionaries and G2Ps.
The Arabic dictionary is from the Qatari Arabic Corpus~\cite{Elmahdy14},
the Dutch dictionary is from CELEX v2~\cite{Baayen96},
the Hungarian dictionary was provided by BUT~\cite{Grezl14},
the Cantonese dictionary is from $I^2R$,
and the Mandarin dictionary is from CALLHOME~\cite{LDC96}.
For
each language, we chose a random 40/10/10 minutes split into training,
development and evaluation sets.
