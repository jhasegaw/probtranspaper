\subsection{EEG Recording and Analysis}
\label{sec:methods_eeg}

To compute distinctive feature weights for the misperception
transducer shown in Eqs.~(\ref{eq:dfdist}) and~(\ref{eq:eegdist}),
cortical activity in response to non-native phones was recorded by
an EEG. Signals were acquired using a BrainVision actiCHamp 
system with 64 channels and 1000 Hz sampling frequency.
All procedures were approved by the University of Washington Institutional
Review Board.

%% This next section, including tab:m2o, seems like more detail than
%% is necessary; the explanation above as to **why** we chose Hindi and
%% Dutch (combined with tab:eegphones) seems sufficient here, without
%% going into the extra details of **how** we settled on those two.
% MH: I feel like it's useful to include at least some of this information.
% I could imagine different ways to define the ``number of many-to-one
% mappings,'' other than the metric listed in eq:m2o.
% Somebody someday might come back and test different versions of this
% metric to see which one is most useful, for some definition of ``useful.''
%% DM: point conceded, though I still think the table is overkill.
Auditory stimuli were consonant-vowel
(CV) syllables representing consonants of three languages: English,
Dutch and Hindi. The inclusion of only two non-English languages 
was dictated by the relatively high number of
repetitions needed for good signal-to-noise ratio from averaged
EEG recordings. The choice of Dutch and Hindi was made based on language
phonological similarity, defined as the number of many-to-one mappings
($N_{M2O}$) between the English phoneme inventory
and the non-English phoneme inventory.
Many-to-one mappings are expected to pose a problem for the 
non-native transcription task being modeled by the misperception 
transducer, so to test the contribution of EEG we chose languages that 
differed greatly in this property. 
Using distinctive feature representations of the phonemes in each
inventory from the PHOIBLE database~\cite{phoible}, a many-to-one 
mapping was defined by finding, for each
non-English phoneme $\phi$, the English phoneme $\psi^*(\phi)$ to which
it is most similar.
%\begin{equation}
%  \psi^*(\phi) = \argmin \sum_k \delta\left(f_k(\psi)\ne f_k(\phi)\right)
%\end{equation}
The number of many-to-one collisions is then defined as
\begin{equation}
  N_{M2O}=\frac{1}{|\Omega_\Psi|}\sum_{\phi_1\ne\phi_2}
  \left[\psi^*(\phi_1)=\psi^*(\phi_2)\right]
\label{eq:m2o}
\end{equation}
where $|\Omega_\Psi|$ is the size of the English phoneme inventory,
and $\left[\cdot\right]$ is the unit indicator function.
The frequency of many-to-one mappings is listed in
Table~\ref{tab:m2o} for several languages.
Hindi was chosen for having a large number of
many-to-one mappings with English, while Dutch has relatively few. 
%% rest of this paragraph could probably be cut if necessary
Note that, although Hindi podcasts were not included in the training
data described in Section~\ref{sec:data}, colloquial spoken Hindi and
Urdu are extremely similar phonologically~\cite{Kachru90}, and
considering that the auditory stimuli for the EEG portion of this
experiment are simple CV syllables, it is reasonable to consider Hindi
and Urdu as equivalent for the purpose of computing feature weights for
the misperception transducer.

\begin{table}
  \centerline{\begin{tabular}{|lr|lr|lr|}\hline
    Language & $N_{M2O}$ &
    Language & $N_{M2O}$ &
    Language & $N_{M2O}$ \\ \hline
    spa & 0.862 & yue & 1.280 & cmn & 1.531 \\
    por & 1.152 & jpn & 1.333 & amh & 1.844 \\
    nld & 1.182 & vie & 1.393 & hun & 1.857 \\
    deu & 1.258 & kor & 1.429 & hin & 2.848 \\\hline
  \end{tabular}}
  \vspace*{1mm}
  \caption{Frequency of many-to-one mappings $N_{M2O}$
    between other languages' phoneme inventories and the inventory of
    English. Languages are represented by their ISO 639-3 codes.}
  \label{tab:m2o}
\end{table}

To construct the auditory stimuli, two vowels and several consonants
were selected from the phoneme inventory of each language (18 consonants
for English, 17 for Dutch, and 19 for Hindi). Consonants were chosen
to emphasize differences in the many-to-one relationships
between English-Dutch and English-Hindi, while maintaining roughly equal 
numbers of consonants for each language. The consonants chosen for each 
language are given in Table~\ref{tab:eegphones}; the vowels chosen were 
the same for all three languages (/a/ and /e/).

\begin{table}
  \centering
  \setlength{\tabcolsep}{0.3em}
  \setlength\extrarowheight{2pt}
  \begin{tabular}{|l||cc|cccc|cc|c|c|c|c|c|c|c|}\hline
     & \multicolumn{15}{c|}{Consonants used in the EEG experiment}\\ \hline\hline
    eng & \multicolumn{2}{c|}{p} & \multicolumn{4}{c|}{t} & \multicolumn{2}{c|}{k} & \textipa{tS} & v & \textipa{D} & z & m & \multicolumn{2}{c|}{n} \\\hline
    nld &  \multicolumn{2}{c|}{p} & \multicolumn{4}{c|}{t} & \multicolumn{2}{c|}{\textipa{G}} & & v & & z & m & \multicolumn{2}{c|}{n} \\\hline
    hin &  p & b & \textipa{\|[t} & \textipa{\|[d} & \textipa{\:t} & \textipa{\:d} & k & \textipa{g} & & \textipa{V} & & & m & \textipa{\|[n} & \textipa{\:n} \\\hline\hline
    eng & \multicolumn{2}{c|}{\textipa{p\super h}} & \multicolumn{4}{c|}{\textipa{t\super h}} & \multicolumn{2}{c|}{\textipa{k\super h}} & \textipa{tS\super h} & f & \textipa{T} & \textipa{S} & l & \textipa{\*r} & \\\hline
    nld & \multicolumn{2}{c|}{\textipa{p\super h}} & \multicolumn{4}{c|}{\textipa{t\super h}} & \multicolumn{2}{c|}{\textipa{k\super h}} & \textipa{tS\super h} & f & & \textipa{S} & l & \textipa{\;R} & j \\\hline
    hin & \multicolumn{2}{c|}{\ipa{b\super H}} & \textipa{\|[t\super h} & \textipa{\:t\super h} & \textipa{\textsubbridge{d}\super H} & \textipa{\:d\super H} & \textipa{k\super h} & \textipa{g\super H} & & & & & &&\\\hline
  \end{tabular}
  \vspace*{1mm}
  \caption{Consonant phones used in the EEG experiment represented using
  IPA. Vertical alignment of cells suggests many-to-one mappings
  expected based on distinctive feature values.}
  \label{tab:eegphones}
\end{table}

Two native speakers of each language (one male and one female) were
recorded (44100 Hz sampling frequency, 16 bit depth) speaking multiple 
repetitions of the set of CV syllables for
their language. Three tokens of each unique syllable were excised from
the raw recordings, downsampled to 24414 Hz (for compatibility with the
presentation hardware, Tucker Davis Technologies RP2.1), and RMS normalized.
Recorded syllables had an average duration of 400~ms, and were presented
via headphones to one monolingual American English listener.
The stimuli were presented in 9 blocks of 15 minutes per block, for a
total of 135 minutes.  Syllables were presented in random order with an
inter-stimulus interval of 350~ms. Twenty-one repetitions of each syllable
were presented, for a grand total of 9072 syllable presentations.

%% TODO: get number of ms where epoch started (MM to email GDL)
EEG recordings were divided into 500 ms epochs.
The epoched data were coded with a subset of distinctive features
that minimally defined the phoneme contrasts of the English consonants.
Where more than one choice of features was sufficient to define those
contrasts, preference was given to features that reflect differences
in temporal as opposed to spectral features of the consonants, due to
the high fidelity of EEG at reflecting temporal envelope properties of 
speech~\cite{Liberto15}. The final set of features chosen was:
continuant, sonorant, delayed release, voicing, aspiration, labial,
coronal, and dorsal.
% In other words, differences in the temporal amplitude envelope of
% consonants have a better chance of being recoverable after being
% filtered through a human auditory system and cortex than do differences
% that are purely spectral in nature; to the extent that spectral
% information in speech is preserved in an EEG signal, it will have been
% transformed to be spatially coded across populations of neurons.

