%\subsection{Cross-Lingual Baselines}
\subsection{ASR Methods}
\label{sec:mlbaseline}

Automatic speech recognition (ASR) systems were trained
in four languages (hun=Hungarian, cmn=Mandarin, swa=Swahili,
yue=Cantonese), using three different types of transcription.
First, a topline {\sc monolingual} system was trained in each
language using speech transcribed by a native speaker of that
language.  Second, a baseline {\sc CL} (cross-lingual) system was
trained using data from other languages, and tested in the target
language.  Third, the experimental {\sc PT-adapt} system was created
by adapting the cross-lingual system to probabilistic transcriptions
in the target language.  The {\sc monolingal} topline system is
trained using native transcripts, and converted to the phone set of
the test language using the G2Ps described in Sec.~\ref{sec:data}.
These resoures were not available to the {\sc CL} or {\sc PT-adapt}
systems, which were not permitted to use any natively transcribed
training data in the test language.

Audio data, native transcripts, and probabilistic
transcripts are as described in Sec.~\ref{sec:data}.  The {\sc
  monolingual} topline system was trained using 40 minutes of
training data, then stream weights and insertion penalties were
calculated using 10 minutes of development test data.  Monolingual
systems were trained using a maximum likelihood (ML) criterion using
the 40 minute in-language training set: GMM parameters were
initialized using a monophone system trained on the same 40 minutes,
NN parameters were initialized using a restricted Boltzmann machine
trained on five hours of unlabeled audio in the same language.  The
{\sc CL} baseline systems were each trained using 40 minutes of
training data in languages other than the test language.  CL systems
were trained using ML, maximum mutual information (MMI), minimum
phone error rate (MPE), and state-based minimum Bayes risk
(sMBR,~\cite{Gibson06}) training criteria.  The {\sc PT-adapt}
system was initialized using the CL system (ML training), then
adapted to the target language using PTs based on mismatched
crowdsourcing (these transcripts are described in detail in
Sec.~\ref{sec:data}).  Probabilistic transcripts based on EEG were
not used to adapt ASR, because it is not yet possible to use EEG to
generate probabilistic transcripts on a scale sufficient for ASR
adaptation.
    
All systems were trained using the Kaldi~\cite{Kaldi2011}
toolkit. Acoustic features consisted of MFCC (13
features), stacked $\pm 3$ frames ($13\times 7=91$ features),
reduced to 40 dimensions using LDA followed by fMLLR.  GMM-HMM
systems directly observed this 40-dimensional vector; NN-HMM systems
computed fMLLR+d+dd stacked $\pm 5$ frames ($40\times 3\times
11=1320$ features/frame).  All systems used tied triphone acoustic
models, based on a decision tree with 1200 leaves.  Each GMM-HMM
used a library of 8000 Gaussians, shared among the 1200 leaves.
Each NN-HMM used six hidden layers with logistic nonlinearities, and
with 1024 nodes per hidden layer, followed by a softmax output layer
with 1200 nodes.
  


