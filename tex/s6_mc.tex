%\subsection{Mismatched Crowdsourcing}
\label{s6:mc}

%\begin{figure}
%  \centerline{\includegraphics[width=0.7\columnwidth]{../figs/lm_results.pdf}}
%  \vspace*{-0.7cm}
%  \caption{LPER of the 1-best path: a measure of the quality of
%    probabilistic transcripts acquired from mismatched
%    crowdsourcing.  Native transcripts were available in six
%    languages: Swahili (swh), Dutch (nld), Mandarin (cmn), Urdu (urd),
%    Arabic (arb), and Hungarian (hun).  Probabilistic transcripts
%    were decoded using three different methods per language: using a
%    universal phone set (leftmost bar in each language), using a
%    phone set specific to the target language (middle bar in each
%    language), and using a phonotactic language model derived from
%    Wikipedia texts (rightmost bar in each language).}
%  \label{fig:pt_decode_per}
%\end{figure}

The quality of a probabilistic transcript derived from mismatched
crowdsourcing is significantly improved by using a phone language
model during the decoding process ($\rho(\phi)$ in Eq.~(\ref{eq:PT})).
%A crude measure of the quality of the PTs is given by the label phone error
%rate (LPER), which measures the difference
%between $\phi^* = \argmax_{\phi} \rho(\phi|T)$ and a native transcript.
Phone
language models for each target language were computed from Wikipedia
texts using the methods described in Sec.~\ref{sec:trainwithlm}.
Label phone error rate
(LPER) of the 1-best path through the resulting PTs are shown in
Table~\ref{fig:pt_decode_per}, computed with reference to a native
transcript in each language.  As shown, the use of a phone
language model, derived from Wikipedia text, reduces LPER by about 10\%
absolute, in each language.

\begin{table}
\centerline{\begin{tabular}{|c||c|c|c|c|c|c|}\hline
    Method & nld & cmn & urd & arb & hun &swh \\\hline
    Universal set & 87.4 & 88.86 & 97.95 & 79.04 & 92.87 & 88.56 \\
    Target set & 78.12 & 87.4 & 87.81 & 66.39 & 84.78 & 59.83 \\
    Phone bigram & 70.43 & 70.88 & 64.67 & 65.29 & 63.98 & 50.45 \\\hline
\end{tabular}}
\vspace*{1mm}
\caption{Label phone error rate (LPER) of probabilistic transcripts
  for universal phone set, target-language phone set, text-based
  phone bigram.}
\label{fig:pt_decode_per}
\end{table}

%\begin{table}[t]
%\centering
%\begin{tabular}{|c||c|c|c|c|c|c|c|}
%  \hline
%  & \multicolumn{7}{|c|}{Language (ISO 639-3 Code)}\\ \hline
%& arb & yue & nld & hun & cmn & swh & urd \\ \hline\hline
%Dev set (1-best PER) & 65.8 & 66.4 & 68.9 & 63.7 & 70.9 & 47.6 & 67.2 \\
%Eval set (1-best PER) & 66.2 & 67.8 & 70.9 & 63.5 & 69.6 & 50.3 & 70.5 \\\hlineswh	88.05	59.78	48.08

%\end{tabular}
%\caption{Error rates (PER) of probabilistic transcripts computed from
%  mismatched crowdsourcing (non-native human listener): Phone error
%  rate (PER) of the 1-best path through the probabilistic
%  transcript, $\phi^*=\argmax\rho(\phi|T)$, development and
%  evaluation sets.}
%\label{tab:LPER}
%\end{table}

%Table~\ref{tab:LPER} lists LPERs on the development and evaluation
%sets, for all seven languages.
LPER of the 1-best path does not
accurately reflect the extent of information in the PTs that can be
leveraged during ASR adaptation.  Consider, for example, the four
Urdu phones~\ipa{[p,p\textsuperscript{h},b,b\textsuperscript{H}]}.  An attentive
English-speaking transcriber must choose between the two letters
$<$p,b$>$ in order to represent any of these four phones.  The
misperception G2P therefore maps the letters $<$p,b$>$ into a
distribution over the phones~\ipa{[p,p\textsuperscript{h},b,b\textsuperscript{H}]}.
There is no reason to expect that the maximizer of
$\rho(\phi|\lambda)$ is correct, but there is good reason to expect
the correct answer to be a member of a short $N$-best list ($N\le 4$
phones/grapheme).  A fuller picture is therefore obtained by
{\color{blue} pruning the PT to
a small number of paths, then searching for the most correct path
in the pruned PT.  One useful metric is entropy per segment, defined
as $H^{\ell}(\Phi)=-\frac{1}{M}\sum_{m=1}^M\sum_{u} \log_2\rho_{\Phi_m^\ell}(u)$,
e.g., a PT in which every segment has two equally probable options 
would measure $H^\ell(\Phi)=1$.}
Fig.~\ref{fig:listPER}
shows the trend of LPER (for three languages) obtained by pruning
the PT at several different levels of $H^{\ell}(\Phi)$.
LPER rates drop significantly across all languages within 1
bit of entropy per phone, illustrating the extent of information
captured by the PTs.

\begin{figure}[t!]
  %\centerline{\includegraphics[width=0.7\columnwidth]{../figs/ptperfigure.pdf}}
  \input{../figs/listper.tex}
  \vspace*{-0.5cm}
  \caption{LPER plotted against entropy rate estimates of phone sequences in three different languages.}
\label{fig:listPER}
\end{figure}

