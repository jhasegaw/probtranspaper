%\subsection{Mismatched Crowdsourcing}
\label{s6:mc}

The quality of a probabilistic transcript derived from mismatched
crowdsourcing is significantly improved by using a phone language
model during the decoding process ($\rho(\phi)$ in Eq.~(\ref{eq:PT})).
Phone
language models for each target language were computed from Wikipedia
texts using the methods described in Sec.~\ref{sec:trainwithlm}.
Label phone error rate
(LPER) of the 1-best path through the resulting PTs are shown in
Table~\ref{fig:pt_decode_per}, computed with reference to a native
transcript in each language.  As shown, the use of a phone
language model, derived from Wikipedia text, reduces LPER by about 10\%
absolute, in each language.

\begin{table}
\centerline{\begin{tabular}{|c||c|c|c|c|c|c|}\hline
    Method & nld & cmn & urd & arb & hun &swh \\\hline
    Universal set & 87.4 & 88.86 & 97.95 & 79.04 & 92.87 & 88.56 \\
    Target set & 78.12 & 87.4 & 87.81 & 66.39 & 84.78 & 59.83 \\
    Phone bigram & 70.43 & 70.88 & 64.67 & 65.29 & 63.98 & 50.45 \\\hline
\end{tabular}}
\vspace*{1mm}
\caption{Label phone error rate (LPER) of probabilistic transcripts
  for universal phone set, target-language phone set, text-based
  phone bigram.}
\label{fig:pt_decode_per}
\end{table}

LPER of the 1-best path does not
accurately reflect the extent of information in the PTs that can be
leveraged during ASR adaptation.  Consider, for example, the four
Urdu phones~\ipa{[p,p\textsuperscript{h},b,b\textsuperscript{H}]}.  An attentive
English-speaking transcriber must choose between the two letters
$<$p,b$>$ in order to represent any of these four phones.  The
misperception G2P therefore maps the letters $<$p,b$>$ into a
distribution over the phones~\ipa{[p,p\textsuperscript{h},b,b\textsuperscript{H}]}.
There is no reason to expect that the maximizer of
$\rho(\phi|\lambda)$ is correct, but there is good reason to expect
the correct answer to be a member of a short $N$-best list ($N\le 4$
phones/grapheme).  A fuller picture is therefore obtained by
pruning the PT to
a small number of paths, then searching for the most correct path
in the pruned PT.  One useful metric is entropy per segment, defined
as $H^{\ell}(\Phi)=-\frac{1}{M}\sum_{m=1}^M\sum_{u} \log_2\rho_{\Phi_m^\ell}(u)$,
e.g., a PT in which every segment has two equally probable options 
would measure $H^\ell(\Phi)=1$.
Fig.~\ref{fig:listPER}
shows the trend of LPER (for three languages) obtained by pruning
the PT at several different levels of $H^{\ell}(\Phi)$.
LPER rates drop significantly across all languages within 1
bit of entropy per phone, illustrating the extent of information
captured by the PTs.

\begin{figure}[t!]
  \input{../figs/listper.tex}
  \vspace*{-0.5cm}
  \caption{LPER plotted against entropy rate estimates of phone sequences in three different languages.}
\label{fig:listPER}
\end{figure}

