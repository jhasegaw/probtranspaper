%\subsection{ASR Trained Using Probabilistic Transcripts}
\label{ssec:asr}

\setlength{\tabcolsep}{0.1cm}
\begin{table*}[t]
\centerline{\begin{tabular}{| c || c | c c c | c c c | c c c |}\hline
    AM& Monolingual&\multicolumn{6}{c|}{Cross-Lingual ({\sc CL})}
    &\multicolumn{3}{c|}{CL + PT adaptation ({\sc PT-adapt})}
    \\\hline
    LM & Transcript& \multicolumn{3}{c|}{CL}&
    \multicolumn{3}{c|}{Text} &
    \multicolumn{3}{c|}{Text}
    \\\hline
    Training & ML & ML & MPE & sMBR & ML & MPE & sMBR & ML & MPE & sMBR
    \\\hline\hline
    yue & 32.77 (34.61)
    & 79.64 (79.83) & (79.49) & (79.48) 
    & 68.40 (68.35) & (68.02) & (66.94) 
    &  57.20** (56.57)
    &  56.33** (55.70)
    & \textbf{55.97}** (55.51)
    \\
    hun & 39.58 (39.77)
    & 77.13 (77.85) & (78.67) & (78.89)
    & 68.62 (66.90) & (67.09) & (67.41)
    & \textbf{56.98}** (57.26)
    & 57.05** (56.95)
    & 57.05** (57.18)
    \\
    cmn & 32.21 (26.92)
    & 83.28 (82.12) & (81.60) & (81.76)
    & 71.30 (68.66) & (66.35) & (66.02)
    & 58.21** (57.85)
    & 55.17** (54.49)
    & \textbf{55.07}** (54.94)
    \\
    swh & 35.33 (46.51)
    & 82.99 (81.86) & (81.80) & (82.32)
    & 63.04 (64.73) & (63.48) & (62.99)
    & 44.31** (48.88)
    & \textbf{42.80}** (46.77)
    & 43.61** (46.55)
    \\\hline
\end{tabular}}
\vspace*{1mm}
\caption{\label{tab:ptresult} Phone error rate of GMM-HMMs: evaluation test (development test).  AM=Acoustic Model, LM=Language Model, ML=Maximum Likelihood, sMBR=State-based Minimum Bayes Risk.  MAPSSWE w/respect to CL: ** denotes $p<0.001$.}
\end{table*}

\setlength{\tabcolsep}{0.1cm}
\begin{table*}[t]
\centerline{\begin{tabular}{| c || c | c c c | c c c | c | c |}\hline
    AM& Monolingual&\multicolumn{6}{c|}{Cross-Lingual ({\sc CL})}&Self-training
    ({\sc ST})    &CL + PT adaptation ({\sc PT-adapt})
    \\\hline
    LM & Transcript& \multicolumn{3}{c|}{CL}&
    \multicolumn{3}{c|}{Text} & Text & Text
    \\\hline
    Training & ML & ML & MPE & sMBR & ML & MPE & sMBR & ML & ML 
    \\\hline\hline
    yue & 27.67 (28.88)
    & 78.62 (77.58) & (77.86) & (77.92) 
    & 66.59 (65.28) & (65.76) & (65.76)
    & 63.79ns (62.46)
    & \textbf{53.64}** (53.80)
    \\
    hun & 35.87 (36.58)
    & 75.98 (76.44) & (77.56) & (77.61)
    & 66.43 (66.58) & (65.52) & (65.67)
    & 63.53ns (63.50)
    &   \textbf{56.70}** (58.45)
    \\
    cmn & 27.80 (23.96)
    & 81.86 (80.47) & (81.02) & (80.93)
    & 65.77 (64.80) & (63.90) & (63.82)
    & 64.90ns (64.00)
    &   \textbf{54.07}** (53.13)
    \\
    swh & 34.98 (41.47)
    & 82.30 (81.18) & (81.30) & (81.24)
    & 65.30 (65.04) & (63.78) & (63.99)
    & 58.76* (59.81)
    &   \textbf{44.73}** (48.60)
    \\\hline
\end{tabular}}
\vspace*{1mm}
\caption{\label{tab:dnnresult} PER of NN-HMMs: evaluation test (development test).  ML=Maximum Likelihood, MPE=minimum phone error, sMBR=state-level Minimum Bayes Risk.  MAPSSWE w/respect to CL AM w/ text-based LM: * means $p<0.002$, ns=not significant. ** denotes a score lower than both CL and ST at $p<0.001$.}
\end{table*}

%This section demonstrates that PT adaptation improves the
%generalization capability of cross-lingual ASR to an unseen target
%language.  Adaptation to ASR-derived PTs (self-training) significantly
%reduces PER, as has been previously
%reported~\cite{vesely2013-semi}. PTs derived from human mismatched
%crowdsourcing provide significant further PER reduction.

Evaluation test PER of each experimental system (columns
{\sc ST} and {\sc PT-adapt}, 20 systems) was compared to evaluation test PER of
the corresponding {\sc CL} system ({\sc ML} training, {\sc Text} LM)
using the MAPSSWE test of the {\tt sc\_stats} tool~\cite{Pallet90}.
Each neural net {\sc PT-adapt} system was also compared
to the corresponding {\sc ST} system (4 comparisons).  There are 
therefore 24 independent statistical comparisons in
Tables~\ref{tab:ptresult} and~\ref{tab:dnnresult}; the study-corrected
significance level is $0.05/24=0.002$.

Self-training was only performed
using NN systems; no self-training of GMMs was performed, because
previous studies~\cite{Huang2013} reported it to be less effective.
The Swahili {\sc ST} system was judged significantly better
than {\sc CL} at a level of $p=0.002$ (denoted *); the Cantonese,
Mandarin and
Hungarian {\sc ST} systems were not significantly better
than {\sc CL} at this level.

The relative reductions in PER
of the {\sc PT-adapt} system
compared to both {\sc CL} and {\sc ST} baselines
were all statistically
significant at $p<0.001$ (denoted **).  This suggests that adaptation
with PTs is providing more information than that obtained by model
self-training alone.

PT-adapt GMM-HMM systems were trained using four
different training criteria: ML, MMI, MPE and sMBR.  MMI training
consistently underperformed MPE and sMBR, and is therefore not shown.
MPE training of {\sc PT-adapt} systems improves their PER by a little
more than 1\% on average, comparable to the improvement provided
to {\sc CL} baseline systems.

PER improvements for Swahili are larger than for the other three
languages.
We conjecture this may be due to the relatively good mapping between
Swahili's phone inventory and that of English. For example: all Swahili
vowel qualities are also found in English, and the Swahili phonemes 
that would be unfamiliar to an English speaker (prenasalized stops, 
palatal consonants) have representations in English orthography that are 
fairly natural (``mb'', ``nd'', etc. for prenasalized stops; ``tya'', 
``chya'', ``nya'', etc. for palatals). In contrast: Mandarin, 
Cantonese, and Hungarian each have at least two vowel qualities not 
found in English; Mandarin and 
Cantonese have many diphthongs not found in English; and some of the 
consonant phonemes (e.g., Mandarin retroflexes) do not have 
representations in English orthography that are obvious or 
straightforward.
