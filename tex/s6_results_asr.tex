\subsection{ASR Trained Using Probabilistic Transcripts}
\label{ssec:asr}

\setlength{\tabcolsep}{0.1cm}
\begin{table*}[t]
\centerline{\begin{tabular}{| c || c | c c c | c c c | c | c c c |}\hline
    Acoustic& Monolingual&\multicolumn{6}{c|}{Cross-Lingual}&Self-training
    &\multicolumn{3}{c|}{CL + PT adaptation}
    \\
    Model&&\multicolumn{6}{c|}{({\sc CL})}&
    ({\sc ST})&\multicolumn{3}{c|}{({\sc PT-adapt})}
    \\\hline
    Language & Transcript& \multicolumn{3}{c|}{CL}&
    \multicolumn{3}{c|}{Text} & Text &
    \multicolumn{3}{c|}{Text}
    \\
    Model &&&&&&&&&&&
    \\\hline\hline
    Training & ML & ML & MPE & sMBR & ML & MPE & sMBR & ML & ML & MPE & sMBR
    \\\hline
    \multicolumn{12}{|l|}{GMM-HMM}
    \\\hline
    yue & 32.77 (34.61)
    & 79.64 (79.83) & (80.42) & (80.44) 
    & 68.40 (68.35) & (69.07) & (68.13) 
    &
    &  \textbf{57.20$\dagger$ (56.57)}
    &  \textbf{56.33$\dagger$ (55.70)}
    & \textbf{55.97$\dagger$ (55.51)}
    \\
    hun & 39.58 (39.77)
    & 77.13 (77.85) & &
    & 68.62 (66.90) & &
    &
    & \textbf{56.98$\dagger$ (57.26)}
    & \textbf{57.05$\dagger$ (56.95)}
    & \textbf{57.05$\dagger$ (57.18)}
    \\
    cmn & 32.21 (26.92)
    & 83.28 (82.12) & &
    & 71.30 (68.66) & &
    &
    & \textbf{58.21$\dagger$ (57.85)}
    & \textbf{55.17$\dagger$ (54.49)}
    & \textbf{55.07$\dagger$ (54.94)}
    \\
    swh & 35.33 (46.51)
    & 82.99 (81.86) & &
    & 63.04 (64.73) & &
    & 
    & \textbf{44.31$\dagger$ (48.88)}
    & \textbf{42.80$\dagger$ (46.77)}
    & \textbf{43.61$\dagger$ (46.55)}
    \\\hline\hline
    \multicolumn{12}{|l|}{NN-HMM} \\\hline
    yue & 27.67 (28.88)
    & 78.62 (77.58) && (77.92) 
    & 66.59 (65.41) && (65.76)
    & 63.79ns (62.46)
    & \textbf{53.64$\dagger$ (53.80)} &&
    \\
    hun & 35.87 (36.58)
    & 75.98 (76.44) &&
    & 66.43 (67.18) &&
    & 63.53ns (63.50)
    &   \textbf{56.70$\dagger$ (58.45)} &&
    \\
    cmn & 27.80 (23.96)
    & 81.86 (80.47) &&
    & 65.77 (64.80) &&
    & 64.90* (64.00)
    &   \textbf{54.07$\dagger$ (53.13)} &&
    \\
    swh & 34.98 (41.47)
    & 82.30 (81.18) &&
    & 65.30 (65.11) &&
    & 58.76** (59.81)
    &   \textbf{44.73$\dagger$ (48.60)} &&
    \\\hline
\end{tabular}}
\vspace*{1mm}
\caption{\label{tab:ptresult} PERs on the evaluation and development sets (development in parentheses) before and after adaptation with PTs.  ML=Maximum Likelihood, sMBR=Structural Minimum Bayes Risk.  MAPSSWE significance testing with respect to CL acoustic model with text-based language model: * means $p\le 0.003$, ** means $p<0.001$, ns means not significant. $\dagger$ denotes a score lower than both CL and ST baselines at $p<0.001$.}
\end{table*}

%This section demonstrates that PT adaptation improves the
%generalization capability of cross-lingual ASR to an unseen target
%language.  Adaptation to ASR-derived PTs (self-training) significantly
%reduces PER, as has been previously
%reported~\cite{vesely2013-semi}. PTs derived from human mismatched
%crowdsourcing provide significant further PER reduction.

Table~\ref{tab:ptresult} presents phone error rates (PERs) on the
evaluation (and development) sets for four different languages. The
column titled {\sc CL} lists cross-lingual baseline error rates.
The column labeled {\sc ST} lists the
PERs of self-trained ASR systems.
The column headed {\sc PT-adapt} in Table~\ref{tab:ptresult} lists
PERs from {\sc CL} ASR systems that have been adapted to PTs in the target
language.

Self-training was only performed
using NN systems; no self-training of GMMs was performed, because
previous studies~\cite{Huang2013} reported it to be less effective.
Differences between the evaluation set PERs of {\sc ST} and {\sc CL}
systems were tested for statistical significance using the MAPSSWE
test of the {\tt sc\_stats} tool~\cite{Pallet90}.  There are 20
independent statistical comparisons in Table~\ref{tab:ptresult}; the
study-corrected significance level of $0.05/20=0.0025$ was rounded up
to $0.003$ because {\tt sc\_stats} only provides three significant
figures.  The Mandarin {\sc ST} system was judged significantly better
than {\sc CL} at a level of $p=0.003$ (denoted *), and the Swahili
system at a level of $p<0.001$ (denoted **); the Cantonese and
Hungarian {\sc ST} systems were judged to be not significantly better
than {\sc CL}.

%We observe substantial PER improvements using {\sc PT-adapt}
%over {\sc CL} across all four languages. We also find that PT
%adaptation consistently outperforms the {\sc ST} systems for all four
%languages.
The relative reductions in PER
of the {\sc PT-adapt} system
compared to both {\sc CL} and {\sc ST} baselines
%are listed in the last two columns.  Reductions on the evaluation set
were tested for statistical significance using the MAPSSWE test of the
{\tt sc\_stats} tool.  All differences were found to be statistically
significant at $p<0.001$ (denoted $\dagger$).  This suggests that adaptation
with PTs is providing more information than that obtained by model
self-training alone.

%It is also interesting that
PER improvements for Swahili are larger than for the other three
languages. %We conjecture this may be partly because Swahili's
%orthography is based on the Roman alphabet, unlike the other three
%languages. Since the mismatched transcripts also used the Roman
%alphabet, the PTs derived from them may more closely resemble the
%native Swahili transcripts (from which the phonetic transcripts
%are derived)
%% DM: the above point (commented out) misses the mark. The influence of
%% orthography here lies in what sounds English speakers typically use
%% roman letters to represent. Swahili happens to have a phone set that
%% can be represented with the roman alphabet in a way that makes 
%% pretty good sense to a native English speaker, and the things that
%% are phonologically weird about Swahili don't cause problems. In
%% contrast, the roman alphabet as used by English speakers doesn't have
%% a good way to handle the things about Mandarin, Cantonese, or
%% Hungarian that are phonologically weird (from an English perspective)
%% e.g., unfamiliar vowel qualities like front rounded vowels, See
%% below for what I think is a more plausible explanation of why PT
%% worked well for Swahili, and an additional point about many-to-one
%% correspondences in the perceptual domain.
We conjecture this may be due to the relatively good mapping between
Swahili's phone inventory and that of English. For example: all Swahili
vowel qualities are also found in English, and the Swahili phonemes 
that would be unfamiliar to an English speaker (prenasalized stops, 
palatal consonants) have representations in English orthography that are 
fairly natural (``mb'', ``nd'', etc. for prenasalized stops; ``tya'', 
``chya'', ``nya'', etc. for palatals). In contrast: Mandarin, 
Cantonese, and Hungarian each have at least two vowel qualities not 
found in English; Mandarin and 
Cantonese have many diphthongs not found in English; and some of the 
consonant phonemes (e.g., Mandarin retroflexes) do not have 
representations in English orthography that are obvious or 
straightforward. %Another possible source of the difference would be if
%Swahili phonemes had (on average) higher perceptual discriminability 
%(to English speakers) than the phonemes of other languages tested. In
%other words, there may have been fewer many-to-one mappings in Swahili
%than in the other languages.
%(Hungarian's contrastive vowel length is one
%example of a likely source of perceptual noise in the mismatched 
%transcripts).
%Both of these factors (orthographic and 
%perceptual) may have led to less label noise in the Swahili PT than in 
%the PTs of the other languages. 

It is also useful to compare the performance of GMM-HMM and NN-HMM
systems.  In the {\sc CL} setting, an ASR trained
using six languages is then applied to an unseen seventh language,
without adaptation; in this setting, the NN consistently outperforms
the GMM.  In the {\sc PT-adapt} setting, GMMs and NNs are
adapted using PTs in the target language.  PT adaptation improves the
performance of both types of ASR, but the
adapted NN does not consistently outperform the GMM across all
tested languages.

