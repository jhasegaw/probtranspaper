%\subsection{Cross-Lingual Baseline}
\subsection{ASR Results}
\label{s6:mlbaseline}

Table~\ref{tab:ptresult} presents phone error rates (PERs) on the
evaluation (and development) sets for four different languages.
{\color{blue} The first column shows the
phone error rate (PER) of monolingual topline systems: evaluation test
results are followed by development test results in parentheses.}
The
column titled {\sc CL} lists cross-lingual baseline error rates.
The column labeled {\sc ST} lists the
PERs of self-trained ASR systems.
The column headed {\sc PT-adapt} in Table~\ref{tab:ptresult} lists
PERs from {\sc CL} ASR systems that have been adapted to PTs in the target
language.
{\color{blue} Phone error rates are reported instead of word error
rates because, in order to compute a word error rate, it is necessary
to have either native transcriptions in the target language (thereby
permitting the training of a grapheme-based recognizer) or a
pronunciation lexicon in the target language.  These resources are
used by the monolingual topline, but not by any of the baseline or
experimental systems.}

%The first four columns of
%Table~\ref{tab:ptresult}  

%compare a monolingual ASR with phone language model based on
%monolingual transcripts, a 
%cross-lingual ASR using universal phone set and
%phone language model, and a cross-lingual ASR using a
%phone language model based on language-dependent Wikipedia texts.

%Without a language specific phone set
%and phone language model, it is hard for a cross-lingual system to
%generalize to an unseen language.  This is true even if the system has
%seen closely related languages such as Mandarin when tested on
%Cantonese.  
%
%\begin{table*}
%\begin{center}
%\begin{tabular}{|c|c|c|cccc|}
%\hline
%data & acoustic & language & yue & hun & cmn & swh \\
% & model & model &  & & & \\
%\hline
%cross-lingual & GMM & cross-lingual & 79.64 (79.83) & 77.13 (77.85) & 83.28 (82.12) & 82.99 (81.86) \\
%cross-lingual & NN & cross-lingual & 78.62 (77.58) & 75.98 (76.44) & 81.86 (80.47) & 82.30 (81.18) \\
%cross-lingual & GMM & text & 68.40 (68.35) & 68.62 (66.90) & 71.30 (68.66) & 63.04 (64.73) \\
%cross-lingual & NN & text & 66.54 (65.28) & 66.08 (66.58) & 65.77 (64.80) & 64.75 (65.04) \\
%\hline
%monolingual & GMM & transcript & 32.77 (34.61) & 39.58 (39.77) & 32.21 (26.92) & 35.33 (46.51) \\
%monolingual & NN & transcript & 27.67 (28.88) & 35.87 (36.58) & 27.80 (23.96) & 34.98 (41.47) \\
%\hline
%\end{tabular}
%\vspace*{1mm}
%\caption{\label{tab:results} PERs of unadapted cross-lingual
%  and monolingual ASR on
%  the evaluation sets (development sets are in parentheses).
%  Text-based language models are
%  trained using Wikipedia.
%  Transcript-based
%  language models are based on
%  native transcripts of the training data.}
%\end{center}
%\end{table*}

The monolingual ASR is trained using only 40 minutes of audio and
transcript data per language, but performs reasonably well (31.58\%
average PER, NN-HMM).  The cross-lingual ASRs, however, perform poorly.
%From the comparison of different baseline systems, we can reach the
%following conclusions.  First, even with only 40 minutes of
%training data, a NN is able to outperform a GMM.  Second,
%however, the standard speech pipeline performs poorly on unseen languages.
Using a language-specific phonotactic language model gives
significant improvement over the language-independent phonotactic
model, but significantly underperforms a system that
has seen the test language during training.  This is true
even if the system has seen closely related languages during training:
the Cantonese cross-lingual system has seen Mandarin during training,
and the Mandarin system has seen Cantonese during training, but
neither system is able to generalize well from its six training languages to
its test language.
{\color{blue} Three different types of discriminative training were tested.
 Maximum mutual information (MMI) performs consistently worse than
 minimum phone error rate (MPE) and structural minimum Bayes risk (sMBR),
 and is therefore not listed in Table~\ref{tab:ptresult}.
 Averaged across all languages and systems shown in Table~\ref{tab:ptresult},
 the development-test PERs of ML, MPE, and sMBR training are
 73.43\%, 73.04\%, and 72.98\% respectively; differences are not
 statistically significant, therefore only the ML system was tested on
 evaluation test data.}

