%\subsection{Cross-Lingual Baseline}
\subsection{ASR Results}
\label{s6:mlbaseline}

Tables~\ref{tab:ptresult} and~\ref{tab:dnnresult} present phone error
rates (PERs) for four different languages.  {\color{blue} The first
  column shows the phone error rate (PER) of monolingual topline
  systems: evaluation test results are followed by development test
  results in parentheses.}  The column titled {\sc CL} lists
cross-lingual baseline error rates.  The column labeled {\sc ST} lists
the PERs of self-trained ASR systems.  The column headed {\sc
  PT-adapt} in Table~\ref{tab:ptresult} lists PERs from {\sc CL} ASR
systems that have been adapted to PTs {\color{blue} derived from
  mismatched crowdsourcing. Phone error rates are reported instead of
  word error rates because, in order to compute a word error rate, it
  is necessary to have either native transcriptions in the target
  language (thereby permitting the training of a grapheme-based
  recognizer) or a pronunciation lexicon in the target language.
  These resources are used by the monolingual topline, but not by any
  of the baseline or experimental systems.}

%The first four columns of
%Table~\ref{tab:ptresult}  

%compare a monolingual ASR with phone language model based on
%monolingual transcripts, a 
%cross-lingual ASR using universal phone set and
%phone language model, and a cross-lingual ASR using a
%phone language model based on language-dependent Wikipedia texts.

%Without a language specific phone set
%and phone language model, it is hard for a cross-lingual system to
%generalize to an unseen language.  This is true even if the system has
%seen closely related languages such as Mandarin when tested on
%Cantonese.  
%
%\begin{table*}
%\begin{center}
%\begin{tabular}{|c|c|c|cccc|}
%\hline
%data & acoustic & language & yue & hun & cmn & swh \\
% & model & model &  & & & \\
%\hline
%cross-lingual & GMM & cross-lingual & 79.64 (79.83) & 77.13 (77.85) & 83.28 (82.12) & 82.99 (81.86) \\
%cross-lingual & NN & cross-lingual & 78.62 (77.58) & 75.98 (76.44) & 81.86 (80.47) & 82.30 (81.18) \\
%cross-lingual & GMM & text & 68.40 (68.35) & 68.62 (66.90) & 71.30 (68.66) & 63.04 (64.73) \\
%cross-lingual & NN & text & 66.54 (65.28) & 66.08 (66.58) & 65.77 (64.80) & 64.75 (65.04) \\
%\hline
%monolingual & GMM & transcript & 32.77 (34.61) & 39.58 (39.77) & 32.21 (26.92) & 35.33 (46.51) \\
%monolingual & NN & transcript & 27.67 (28.88) & 35.87 (36.58) & 27.80 (23.96) & 34.98 (41.47) \\
%\hline
%\end{tabular}
%\vspace*{1mm}
%\caption{\label{tab:results} PERs of unadapted cross-lingual
%  and monolingual ASR on
%  the evaluation sets (development sets are in parentheses).
%  Text-based language models are
%  trained using Wikipedia.
%  Transcript-based
%  language models are based on
%  native transcripts of the training data.}
%\end{center}
%\end{table*}

The monolingual ASR is trained using only 40 minutes of audio and
transcript data per language, but performs reasonably well (31.58\%
average PER, NN-HMM).  The cross-lingual ASRs, however, perform poorly.
%From the comparison of different baseline systems, we can reach the
%following conclusions.  First, even with only 40 minutes of
%training data, a NN is able to outperform a GMM.  Second,
%however, the standard speech pipeline performs poorly on unseen languages.
Using a text-based phone bigram (denoted {\sc TEXT}) gives significant
improvement over a cross-language phone bigram (denoted {\sc CL}),
but significantly underperforms a system that has seen the test
language during training.  This is true even if the system has seen
closely related languages during training: the Cantonese cross-lingual
system has seen Mandarin during training, and the Mandarin system has
seen Cantonese during training, but neither system is able to
generalize well from its six training languages to its test language.
{\color{blue} Three different types of discriminative training were
  tested.  MMI performs consistently worse than MPE and sMBR, and is
  therefore not listed in Table~\ref{tab:ptresult}.  Averaged across
  all languages and systems shown in Table~\ref{tab:ptresult}, the
  development-test PERs of ML, MPE, and sMBR training are 73.43\%,
  73.04\%, and 72.98\% respectively; differences are not statistically
  significant, therefore only the ML system was tested on evaluation
  test data.}

